{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22769ab3",
   "metadata": {},
   "source": [
    "# Data Feature Engineering Notebook\n",
    "\n",
    "This notebook demonstrates the **feature extraction pipeline** that transforms raw prompts into numerical features for machine learning.\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "```\n",
    "Raw Prompts → Clean Text → Tokenize → Extract Features → Save CSV\n",
    "```\n",
    "\n",
    "## Core Features (Calibrated Model - December 2025)\n",
    "\n",
    "The production model uses **6 core features** that were found to have the strongest correlation with energy consumption:\n",
    "\n",
    "| Feature | Description | Correlation |\n",
    "|---------|-------------|-------------|\n",
    "| **token_count** | Number of tokens | 0.946 |\n",
    "| **word_count** | Number of words | 0.915 |\n",
    "| **char_count** | Total characters | High |\n",
    "| **complexity_score** | Linguistic complexity | 0.516 |\n",
    "| **avg_word_length** | Mean word length | Medium |\n",
    "| **avg_sentence_length** | Mean sentence length | Medium |\n",
    "\n",
    "## Model Performance (After Calibration)\n",
    "| Metric | Achieved |\n",
    "|--------|----------|\n",
    "| **R² Score** | 0.9813 |\n",
    "| **MAPE** | 6.8% |\n",
    "| **Prediction Bias** | 0.9988 |\n",
    "\n",
    "## Output Files\n",
    "- `data/processed/features_df.csv` - Extracted features\n",
    "- `data/synthetic/hybrid_training_data.csv` - Calibrated training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76db6974a71861c4",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "\n",
    "Load required packages:\n",
    "- **pandas/numpy**: Data manipulation\n",
    "- **transformers**: HuggingFace tokenizer (DistilBERT)\n",
    "- **nltk**: Natural language toolkit for stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4298d1c545a4b68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bece8740ea32f162",
   "metadata": {},
   "source": [
    "## 2. Initialize Tokenizer & Stopwords\n",
    "\n",
    "Set up the NLP tools:\n",
    "- **DistilBERT tokenizer**: Converts text to tokens (subword units)\n",
    "- **Stopwords**: Common words like \"the\", \"is\", \"a\" (to calculate stopword ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f172866605db5901",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\", quiet=True)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467b2376a0018ce4",
   "metadata": {},
   "source": [
    "## 3. Load and Clean Raw Data\n",
    "\n",
    "Load prompts from `data/raw/raw_prompts.csv` and apply basic cleaning:\n",
    "- Remove extra whitespace\n",
    "- Filter out very short strings (< 5 characters)\n",
    "- Drop null values\n",
    "\n",
    "**Expected Output**: ~50 rows loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0502284cf43713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    #Basic cleaning of data: remove extra spaces and very short string\n",
    "\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text if len(text) > 5 else None\n",
    "\n",
    "def load_clean_raw(csv_path=\"../../data/raw/raw_prompts.csv\"):\n",
    "    #load csv and clean prompts\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"prompt\"] = df[\"prompt\"].apply(clean_text)\n",
    "    df = df.dropna(subset=[\"prompt\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "df_raw = load_clean_raw()\n",
    "print(f\"Raw data loaded: {len(df_raw)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e8c262fa29a60",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering Functions\n",
    "\n",
    "Define the core feature extraction logic:\n",
    "\n",
    "### `compute_feature(prompt, num_layers, training_hours, flops_per_hour)`\n",
    "\n",
    "Extracts these features:\n",
    "- **token_count**: DistilBERT tokenization count\n",
    "- **char_count**: Total characters\n",
    "- **punct_ratio**: Punctuation / total chars\n",
    "- **avg_word_length**: Mean word length\n",
    "- **stopword_ratio**: Common words / total words\n",
    "- **flops_per_layer**: Compute per model layer\n",
    "- **training_efficiency**: Hours per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6421c0c70cbaf407",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_feature(prompt, num_layers, training_hours, flops_per_hour):\n",
    "    #Compute features for a given prompt and model params\n",
    "\n",
    "\n",
    "\n",
    "    word = prompt.split()\n",
    "    chars = len(prompt)\n",
    "\n",
    "    #token count using tokenizer\n",
    "    token_counter = len(tokenizer.encode(prompt))\n",
    "\n",
    "    #ratio of punctiuatioin characters\n",
    "    punct_ratio = sum(1 for c in prompt if c in \".,!?;:\") /max(chars,1)\n",
    "\n",
    "    #Average word length\n",
    "    avg_word_len = sum(len(w) for w in word) /max(len(word),1)\n",
    "\n",
    "\n",
    "    #Ratio of stopwrods\n",
    "\n",
    "    stopword_ratio = sum(1 for w in word if w.lower() in stop_words) /max(len(word),1)\n",
    "\n",
    "    #Derived numeric features\n",
    "    flops_per_layer = flops_per_hour / max(num_layers,1)\n",
    "    training_efficiency = training_hours / max(num_layers,1)\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"token_count\": token_counter,\n",
    "        \"char_count\": chars,\n",
    "        \"punct_ratio\": punct_ratio,\n",
    "        \"avg_word_length\": avg_word_len,\n",
    "        \"stopword_ratio\": stopword_ratio,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"training_hours\": training_hours,\n",
    "        \"flops_per_hour\": flops_per_hour,\n",
    "        \"flops_per_layer\": flops_per_layer,\n",
    "        \"training_efficiency\": training_efficiency,\n",
    "\n",
    "    }\n",
    "\n",
    "def create_feature_pipeline(df, num_layers_list, training_hours_list, flops_per_hour_list):\n",
    "    rows = []\n",
    "    for i, row in df.iterrows():\n",
    "        features = compute_feature(\n",
    "            row[\"prompt\"], num_layers_list[i], training_hours_list[i], flops_per_hour_list[i]\n",
    "        )\n",
    "        rows.append(features)\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3870a63f87138501",
   "metadata": {},
   "source": [
    "## 5. Generate Model Parameters & Compute Features\n",
    "\n",
    "Simulate LLM parameters for each prompt:\n",
    "- **num_layers**: Random 4-48 (typical range for transformer models)\n",
    "- **training_hours**: Random 0.5-20 hours\n",
    "- **flops_per_hour**: Random 1B - 1T FLOPs\n",
    "\n",
    "**Output**: Saves to `data/processed/features_df.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f39c24f4e72da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = len(df_raw)\n",
    "layers = np.random.randint(4, 48, size=n)\n",
    "hours = np.random.uniform(0.5, 20, size=n)\n",
    "flops = np.random.uniform(1e9, 1e12, size=n)\n",
    "\n",
    "features_df = create_feature_pipeline(df_raw, layers, hours, flops)\n",
    "\n",
    "# Save processed dataset\n",
    "os.makedirs(\"../../data/processed\", exist_ok=True)\n",
    "features_df.to_csv(\"../../data/processed/features_df.csv\", index=False)\n",
    "print(f\"Processed dataset saved with {len(features_df)} rows\")\n",
    "features_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d04ee7608a0308",
   "metadata": {},
   "source": [
    "## 6. Generate Synthetic Energy Labels\n",
    "\n",
    "Create energy consumption labels using a formula:\n",
    "\n",
    "```python\n",
    "energy_kwh = 0.5 + \n",
    "    token_count * 0.003 +           # More tokens = more energy\n",
    "    avg_word_length * 0.10 +        # Longer words = more processing\n",
    "    num_layers * 0.01 +             # Deeper models = more compute\n",
    "    log10(flops_per_hour) * 0.05 +  # Higher FLOPs = more energy\n",
    "    random_noise                     # Realistic variation\n",
    "```\n",
    "\n",
    "**Output**: Saves to `data/synthetic/energy_dataset.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ede2ba3420909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_energy_data(df):\n",
    "    \"\"\"Generate synthetic energy labels for testing.\"\"\"\n",
    "    # Create synthetic energy consumption based on features\n",
    "    energy = (\n",
    "        0.5 +  # Base energy\n",
    "        df[\"token_count\"] * 0.003 +\n",
    "        df[\"avg_word_length\"] * 0.10 +\n",
    "        df[\"num_layers\"] * 0.01 +\n",
    "        np.log10(df[\"flops_per_hour\"] + 1) * 0.05 +\n",
    "        np.random.normal(0, 0.02, size=len(df))  # Small noise\n",
    "    )\n",
    "    \n",
    "    df_energy = df.copy()\n",
    "    df_energy[\"energy_kwh\"] = np.maximum(0.01, energy)  # Ensure positive\n",
    "    \n",
    "    os.makedirs(\"../../data/synthetic\", exist_ok=True)\n",
    "    df_energy.to_csv(\"../../data/synthetic/energy_dataset.csv\", index=False)\n",
    "    return df_energy\n",
    "\n",
    "# Generate energy labels\n",
    "energy_df = generate_energy_data(features_df)\n",
    "print(f\"Synthetic energy dataset saved with {len(energy_df)} rows\")\n",
    "print(f\"Energy range: {energy_df['energy_kwh'].min():.4f} - {energy_df['energy_kwh'].max():.4f} kWh\")\n",
    "energy_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a494f6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the feature engineering pipeline:\n",
    "\n",
    "| Step | Input | Output |\n",
    "|------|-------|--------|\n",
    "| 1. Load | raw_prompts.csv | Cleaned DataFrame |\n",
    "| 2. Tokenize | Text prompts | Token counts |\n",
    "| 3. Extract | Prompts + params | 11 features |\n",
    "| 4. Generate | Features | Energy labels |\n",
    "\n",
    "### Files Generated\n",
    "- `data/processed/features_df.csv` - Feature matrix\n",
    "- `data/synthetic/energy_dataset.csv` - Features + energy labels\n",
    "\n",
    "### Next Steps\n",
    "Run `Energy Prediction ML.ipynb` to train models on this data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
