measurement_id,prompt,category,token_count,word_count,char_count,complexity_score,avg_word_length,avg_sentence_length,model_name,model_params_millions,energy_kwh,energy_joules,duration_seconds,carbon_kg,cpu_model,gpu_available,timestamp
M0001,What is machine learning?,simple,6,4,25,0.5,5.5,4,t5-small,60.0,2.1540243049255674e-06,7.754487497732043,0.31017949990928173,8.61609721970227e-07,estimated,False,2025-12-04T17:16:26.772736
M0002,Define artificial intelligence.,simple,6,3,31,0.5,9.666666666666666,3,t5-small,60.0,1.1849673610413447e-06,4.265882499748841,0.17063529998995364,4.739869444165379e-07,estimated,False,2025-12-04T17:16:26.994402
M0003,Explain Python briefly.,simple,5,3,23,0.5,7.0,3,t5-small,60.0,1.0177444443494702e-06,3.663879999658093,0.14655519998632371,4.070977777397881e-07,estimated,False,2025-12-04T17:16:27.192352
M0004,How can I improve my Python programming skills effectively and become a better developer?,medium,17,14,89,0.5,5.428571428571429,14,t5-small,60.0,4.291470833575456e-06,15.449295000871643,0.6179718000348657,1.7165883334301826e-06,estimated,False,2025-12-04T17:16:27.862114
M0005,"Explain the differences between supervised learning, unsupervised learning, and reinforcement learning.",medium,17,11,103,0.5,8.454545454545455,11,t5-small,60.0,3.3294048609807054e-06,11.985857499530539,0.47943429998122156,1.3317619443922823e-06,estimated,False,2025-12-04T17:16:28.393181
M0006,"Explain how neural networks learn patterns from data through the process of backpropagation, including forward pass, loss calculation, gradient computation, and weight updates. Also describe how different activation functions like ReLU, sigmoid, and tanh affect the learning process.",long,58,38,283,0.5,6.473684210526316,38,t5-small,60.0,7.671072222163073e-06,27.615859999787062,1.1046343999914825,3.0684288888652292e-06,estimated,False,2025-12-04T17:16:29.549611
M0007,"Describe the complete process of training a machine learning model from scratch, including data collection, preprocessing, feature engineering, model selection, hyperparameter tuning, cross-validation, training, evaluation, and deployment to production.",long,49,29,253,0.5,7.758620689655173,29,t5-small,60.0,1.002387569395877e-05,36.08595249825157,1.443438099930063,4.009550277583508e-06,estimated,False,2025-12-04T17:16:31.045347
M0008,"Provide a comprehensive analysis of the transformer architecture including self-attention mechanisms, positional encoding, multi-head attention, layer normalization, residual connections, and the encoder-decoder structure. Explain how these components work together for natural language processing tasks like translation, summarization, and question answering. Also discuss the computational complexity and memory requirements of transformers compared to RNNs and LSTMs.",very_long,89,55,470,0.5,7.5636363636363635,55,t5-small,60.0,1.9147083333033756e-05,68.92949999892153,2.757179999956861,7.658833333213502e-06,estimated,False,2025-12-04T17:16:33.855533
M0009,"Develop a detailed strategy for building a production-ready machine learning pipeline considering all stages from data ingestion and validation, through feature engineering and selection, model training with cross-validation, hyperparameter optimization using grid search or Bayesian methods, model evaluation with appropriate metrics, deployment using containerization and orchestration, monitoring for data drift and model degradation, and establishing feedback loops for continuous improvement.",very_long,85,60,497,0.5,7.3,60,t5-small,60.0,1.479827361052028e-05,53.27378499787301,2.1309513999149203,5.919309444208113e-06,estimated,False,2025-12-04T17:16:36.053776
M0010,"Create an exhaustive comparison of different deep learning frameworks including TensorFlow, PyTorch, JAX, and MXNet across multiple dimensions such as ease of use and learning curve, computational performance and optimization capabilities, distributed training support, mobile and edge deployment options, community support and ecosystem, debugging and profiling tools, and production deployment considerations. Provide specific examples and use cases where each framework excels. Additionally discuss the trade-offs between static and dynamic computation graphs and how this affects model development and debugging. Compare the performance of these frameworks on common benchmarks like ImageNet classification and GLUE language understanding tasks. Discuss the impact of hardware accelerators like GPUs and TPUs on framework choice and how each framework handles automatic mixed precision training for improved throughput.",extra_long,169,121,923,0.5,6.636363636363637,121,t5-small,60.0,1.937024791712045e-05,69.73289250163361,2.7893157000653446,7.74809916684818e-06,estimated,False,2025-12-04T17:16:38.896267
M0011,"Analyze the complete lifecycle of a machine learning project from initial problem formulation and stakeholder requirements gathering, through data acquisition and quality assessment, exploratory data analysis and visualization, feature engineering and selection using statistical methods and domain knowledge, model selection comparing different algorithm families like linear models, tree-based ensembles, neural networks, and support vector machines, hyperparameter optimization strategies including random search, grid search, and Bayesian optimization, model evaluation with cross-validation and proper test set holdout, error analysis and debugging, model interpretation using SHAP values and LIME, deployment strategies for batch and real-time inference, monitoring for performance degradation and data drift, and establishing processes for model retraining and versioning. Also cover the importance of reproducibility through version control of data, code, and models, documentation best practices, and establishing clear communication channels between data scientists and stakeholders throughout the project.",extra_long,192,138,1116,0.5,7.094202898550725,138,t5-small,60.0,1.7113771527268302e-05,61.60957749816589,2.4643830999266356,6.845508610907321e-06,estimated,False,2025-12-04T17:16:41.413786
M0012,"Provide an in-depth technical explanation of how large language models like GPT and BERT work, starting from the fundamental concepts of word embeddings and contextual representations, through the transformer architecture with its self-attention mechanism that allows tokens to attend to all other tokens in the sequence, the role of positional encoding in capturing sequence order information, how multi-head attention enables the model to focus on different aspects of the input simultaneously, the importance of layer normalization and residual connections for stable training of deep networks, the pre-training objectives like masked language modeling for BERT and causal language modeling for GPT, fine-tuning strategies for downstream tasks, techniques for efficient inference like quantization and pruning, and recent advances like instruction tuning and reinforcement learning from human feedback. Additionally explain the scaling laws that govern the relationship between model size, dataset size, and compute budget, and how these findings have influenced the development of increasingly larger models.",extra_long,202,154,1112,0.5,6.2272727272727275,154,t5-small,60.0,2.031704930575668e-05,73.14137750072405,2.925655100028962,8.126819722302672e-06,estimated,False,2025-12-04T17:16:44.392406
M0013,What is machine learning?,simple,6,4,25,0.5,5.5,4,t5-small,60.0,2.1806215275622285e-06,7.850237499224022,0.3140094999689609,8.722486110248915e-07,estimated,False,2025-12-04T17:16:44.757700
M0014,Define artificial intelligence.,simple,6,3,31,0.5,9.666666666666666,3,t5-small,60.0,1.2157368053319967e-06,4.376652499195188,0.17506609996780753,4.862947221327987e-07,estimated,False,2025-12-04T17:16:44.984225
M0015,Explain Python briefly.,simple,5,3,23,0.5,7.0,3,t5-small,60.0,1.026895833395732e-06,3.696825000224635,0.1478730000089854,4.107583333582928e-07,estimated,False,2025-12-04T17:16:45.183786
M0016,How can I improve my Python programming skills effectively and become a better developer?,medium,17,14,89,0.5,5.428571428571429,14,t5-small,60.0,4.4499368054352495e-06,16.019772499566898,0.6407908999826759,1.7799747221740999e-06,estimated,False,2025-12-04T17:16:45.875999
M0017,"Explain the differences between supervised learning, unsupervised learning, and reinforcement learning.",medium,17,11,103,0.5,8.454545454545455,11,t5-small,60.0,2.9978458333062008e-06,10.792244999902323,0.4316897999960929,1.1991383333224804e-06,estimated,False,2025-12-04T17:16:46.359923
M0018,"Explain how neural networks learn patterns from data through the process of backpropagation, including forward pass, loss calculation, gradient computation, and weight updates. Also describe how different activation functions like ReLU, sigmoid, and tanh affect the learning process.",long,58,38,283,0.5,6.473684210526316,38,t5-small,60.0,8.129164583806413e-06,29.26499250170309,1.1705997000681236,3.2516658335225657e-06,estimated,False,2025-12-04T17:16:47.582782
M0019,"Describe the complete process of training a machine learning model from scratch, including data collection, preprocessing, feature engineering, model selection, hyperparameter tuning, cross-validation, training, evaluation, and deployment to production.",long,49,29,253,0.5,7.758620689655173,29,t5-small,60.0,9.518856945053105e-06,34.26788500219118,1.3707154000876471,3.8075427780212422e-06,estimated,False,2025-12-04T17:16:49.006129
M0020,"Provide a comprehensive analysis of the transformer architecture including self-attention mechanisms, positional encoding, multi-head attention, layer normalization, residual connections, and the encoder-decoder structure. Explain how these components work together for natural language processing tasks like translation, summarization, and question answering. Also discuss the computational complexity and memory requirements of transformers compared to RNNs and LSTMs.",very_long,89,55,470,0.5,7.5636363636363635,55,t5-small,60.0,1.2450547917170398e-05,44.821972501813434,1.7928789000725374,4.980219166868159e-06,estimated,False,2025-12-04T17:16:50.851461
M0021,"Develop a detailed strategy for building a production-ready machine learning pipeline considering all stages from data ingestion and validation, through feature engineering and selection, model training with cross-validation, hyperparameter optimization using grid search or Bayesian methods, model evaluation with appropriate metrics, deployment using containerization and orchestration, monitoring for data drift and model degradation, and establishing feedback loops for continuous improvement.",very_long,85,60,497,0.5,7.3,60,t5-small,60.0,1.4650587500202365e-05,52.74211500072852,2.1096846000291407,5.860235000080947e-06,estimated,False,2025-12-04T17:16:53.013641
M0022,"Create an exhaustive comparison of different deep learning frameworks including TensorFlow, PyTorch, JAX, and MXNet across multiple dimensions such as ease of use and learning curve, computational performance and optimization capabilities, distributed training support, mobile and edge deployment options, community support and ecosystem, debugging and profiling tools, and production deployment considerations. Provide specific examples and use cases where each framework excels. Additionally discuss the trade-offs between static and dynamic computation graphs and how this affects model development and debugging. Compare the performance of these frameworks on common benchmarks like ImageNet classification and GLUE language understanding tasks. Discuss the impact of hardware accelerators like GPUs and TPUs on framework choice and how each framework handles automatic mixed precision training for improved throughput.",extra_long,169,121,923,0.5,6.636363636363637,121,t5-small,60.0,1.9855448610744336e-05,71.47961499867961,2.8591845999471843,7.942179444297735e-06,estimated,False,2025-12-04T17:16:55.926804
M0023,"Analyze the complete lifecycle of a machine learning project from initial problem formulation and stakeholder requirements gathering, through data acquisition and quality assessment, exploratory data analysis and visualization, feature engineering and selection using statistical methods and domain knowledge, model selection comparing different algorithm families like linear models, tree-based ensembles, neural networks, and support vector machines, hyperparameter optimization strategies including random search, grid search, and Bayesian optimization, model evaluation with cross-validation and proper test set holdout, error analysis and debugging, model interpretation using SHAP values and LIME, deployment strategies for batch and real-time inference, monitoring for performance degradation and data drift, and establishing processes for model retraining and versioning. Also cover the importance of reproducibility through version control of data, code, and models, documentation best practices, and establishing clear communication channels between data scientists and stakeholders throughout the project.",extra_long,192,138,1116,0.5,7.094202898550725,138,t5-small,60.0,1.6410672916713844e-05,59.078422500169836,2.3631369000067934,6.564269166685538e-06,estimated,False,2025-12-04T17:16:58.342988
M0024,"Provide an in-depth technical explanation of how large language models like GPT and BERT work, starting from the fundamental concepts of word embeddings and contextual representations, through the transformer architecture with its self-attention mechanism that allows tokens to attend to all other tokens in the sequence, the role of positional encoding in capturing sequence order information, how multi-head attention enables the model to focus on different aspects of the input simultaneously, the importance of layer normalization and residual connections for stable training of deep networks, the pre-training objectives like masked language modeling for BERT and causal language modeling for GPT, fine-tuning strategies for downstream tasks, techniques for efficient inference like quantization and pruning, and recent advances like instruction tuning and reinforcement learning from human feedback. Additionally explain the scaling laws that govern the relationship between model size, dataset size, and compute budget, and how these findings have influenced the development of increasingly larger models.",extra_long,202,154,1112,0.5,6.2272727272727275,154,t5-small,60.0,1.946122916625528e-05,70.06042499851901,2.8024169999407604,7.784491666502112e-06,estimated,False,2025-12-04T17:17:01.199047
M0025,What is machine learning?,simple,6,4,25,0.5,5.5,4,t5-small,60.0,1.9442847228169235e-06,6.999425002140924,0.279977000085637,7.777138891267695e-07,estimated,False,2025-12-04T17:17:01.531296
M0026,Define artificial intelligence.,simple,6,3,31,0.5,9.666666666666666,3,t5-small,60.0,1.184217360989553e-06,4.26318249956239,0.1705272999824956,4.736869443958212e-07,estimated,False,2025-12-04T17:17:01.753317
M0027,Explain Python briefly.,simple,5,3,23,0.5,7.0,3,t5-small,60.0,1.053890278045502e-06,3.794005000963807,0.15176020003855228,4.215561112182008e-07,estimated,False,2025-12-04T17:17:01.957003
M0028,How can I improve my Python programming skills effectively and become a better developer?,medium,17,14,89,0.5,5.428571428571429,14,t5-small,60.0,4.310878472299211e-06,15.519162500277162,0.6207665000110865,1.7243513889196847e-06,estimated,False,2025-12-04T17:17:02.629385
M0029,"Explain the differences between supervised learning, unsupervised learning, and reinforcement learning.",medium,17,11,103,0.5,8.454545454545455,11,t5-small,60.0,3.074544444744889e-06,11.0683600010816,0.44273440004326403,1.2298177778979558e-06,estimated,False,2025-12-04T17:17:03.123644
M0030,"Explain how neural networks learn patterns from data through the process of backpropagation, including forward pass, loss calculation, gradient computation, and weight updates. Also describe how different activation functions like ReLU, sigmoid, and tanh affect the learning process.",long,58,38,283,0.5,6.473684210526316,38,t5-small,60.0,8.527090277412854e-06,30.697524998686276,1.227900999947451,3.410836110965142e-06,estimated,False,2025-12-04T17:17:04.403237
M0031,"Describe the complete process of training a machine learning model from scratch, including data collection, preprocessing, feature engineering, model selection, hyperparameter tuning, cross-validation, training, evaluation, and deployment to production.",long,49,29,253,0.5,7.758620689655173,29,t5-small,60.0,8.633333333515717e-06,31.080000000656582,1.2432000000262633,3.453333333406287e-06,estimated,False,2025-12-04T17:17:05.697924
M0032,"Provide a comprehensive analysis of the transformer architecture including self-attention mechanisms, positional encoding, multi-head attention, layer normalization, residual connections, and the encoder-decoder structure. Explain how these components work together for natural language processing tasks like translation, summarization, and question answering. Also discuss the computational complexity and memory requirements of transformers compared to RNNs and LSTMs.",very_long,89,55,470,0.5,7.5636363636363635,55,t5-small,60.0,1.1904899305793354e-05,42.85763750085607,1.7143055000342429,4.7619597223173414e-06,estimated,False,2025-12-04T17:17:07.464051
M0033,"Develop a detailed strategy for building a production-ready machine learning pipeline considering all stages from data ingestion and validation, through feature engineering and selection, model training with cross-validation, hyperparameter optimization using grid search or Bayesian methods, model evaluation with appropriate metrics, deployment using containerization and orchestration, monitoring for data drift and model degradation, and establishing feedback loops for continuous improvement.",very_long,85,60,497,0.5,7.3,60,t5-small,60.0,2.0335627083517868e-05,73.20825750066433,2.9283303000265732,8.134250833407148e-06,estimated,False,2025-12-04T17:17:10.444326
M0034,"Create an exhaustive comparison of different deep learning frameworks including TensorFlow, PyTorch, JAX, and MXNet across multiple dimensions such as ease of use and learning curve, computational performance and optimization capabilities, distributed training support, mobile and edge deployment options, community support and ecosystem, debugging and profiling tools, and production deployment considerations. Provide specific examples and use cases where each framework excels. Additionally discuss the trade-offs between static and dynamic computation graphs and how this affects model development and debugging. Compare the performance of these frameworks on common benchmarks like ImageNet classification and GLUE language understanding tasks. Discuss the impact of hardware accelerators like GPUs and TPUs on framework choice and how each framework handles automatic mixed precision training for improved throughput.",extra_long,169,121,923,0.5,6.636363636363637,121,t5-small,60.0,2.2030705555355074e-05,79.31053999927826,3.1724215999711305,8.81228222214203e-06,estimated,False,2025-12-04T17:17:13.670223
M0035,"Analyze the complete lifecycle of a machine learning project from initial problem formulation and stakeholder requirements gathering, through data acquisition and quality assessment, exploratory data analysis and visualization, feature engineering and selection using statistical methods and domain knowledge, model selection comparing different algorithm families like linear models, tree-based ensembles, neural networks, and support vector machines, hyperparameter optimization strategies including random search, grid search, and Bayesian optimization, model evaluation with cross-validation and proper test set holdout, error analysis and debugging, model interpretation using SHAP values and LIME, deployment strategies for batch and real-time inference, monitoring for performance degradation and data drift, and establishing processes for model retraining and versioning. Also cover the importance of reproducibility through version control of data, code, and models, documentation best practices, and establishing clear communication channels between data scientists and stakeholders throughout the project.",extra_long,192,138,1116,0.5,7.094202898550725,138,t5-small,60.0,1.5632927083061075e-05,56.278537499019876,2.251141499960795,6.253170833224431e-06,estimated,False,2025-12-04T17:17:15.975842
M0036,"Provide an in-depth technical explanation of how large language models like GPT and BERT work, starting from the fundamental concepts of word embeddings and contextual representations, through the transformer architecture with its self-attention mechanism that allows tokens to attend to all other tokens in the sequence, the role of positional encoding in capturing sequence order information, how multi-head attention enables the model to focus on different aspects of the input simultaneously, the importance of layer normalization and residual connections for stable training of deep networks, the pre-training objectives like masked language modeling for BERT and causal language modeling for GPT, fine-tuning strategies for downstream tasks, techniques for efficient inference like quantization and pruning, and recent advances like instruction tuning and reinforcement learning from human feedback. Additionally explain the scaling laws that govern the relationship between model size, dataset size, and compute budget, and how these findings have influenced the development of increasingly larger models.",extra_long,202,154,1112,0.5,6.2272727272727275,154,t5-small,60.0,1.932849930583163e-05,69.58259750099387,2.783303900039755,7.731399722332653e-06,estimated,False,2025-12-04T17:17:18.813054
M0037,What is machine learning?,simple,6,4,25,0.5,5.5,4,t5-small,60.0,2.75037500008087e-06,9.901350000291131,0.39605400001164526,1.100150000032348e-06,estimated,False,2025-12-04T17:17:19.260501
M0038,Define artificial intelligence.,simple,6,3,31,0.5,9.666666666666666,3,t5-small,60.0,1.3340840281064932e-06,4.802702501183376,0.19210810004733503,5.336336112425972e-07,estimated,False,2025-12-04T17:17:19.503882
M0039,Explain Python briefly.,simple,5,3,23,0.5,7.0,3,t5-small,60.0,1.1795305553176957e-06,4.246309999143705,0.1698523999657482,4.718122221270783e-07,estimated,False,2025-12-04T17:17:19.725805
M0040,How can I improve my Python programming skills effectively and become a better developer?,medium,17,14,89,0.5,5.428571428571429,14,t5-small,60.0,4.977968750204632e-06,17.920687500736676,0.716827500029467,1.991187500081853e-06,estimated,False,2025-12-04T17:17:20.494379
M0041,"Explain the differences between supervised learning, unsupervised learning, and reinforcement learning.",medium,17,11,103,0.5,8.454545454545455,11,t5-small,60.0,2.8896208330277457e-06,10.402634998899885,0.4161053999559954,1.1558483332110983e-06,estimated,False,2025-12-04T17:17:20.962581
M0042,"Explain how neural networks learn patterns from data through the process of backpropagation, including forward pass, loss calculation, gradient computation, and weight updates. Also describe how different activation functions like ReLU, sigmoid, and tanh affect the learning process.",long,58,38,283,0.5,6.473684210526316,38,t5-small,60.0,7.228713888455079e-06,26.023369998438284,1.0409347999375314,2.8914855553820317e-06,estimated,False,2025-12-04T17:17:22.055723
M0043,"Describe the complete process of training a machine learning model from scratch, including data collection, preprocessing, feature engineering, model selection, hyperparameter tuning, cross-validation, training, evaluation, and deployment to production.",long,49,29,253,0.5,7.758620689655173,29,t5-small,60.0,8.576541666924539e-06,30.875550000928342,1.2350220000371337,3.4306166667698157e-06,estimated,False,2025-12-04T17:17:23.342565
M0044,"Provide a comprehensive analysis of the transformer architecture including self-attention mechanisms, positional encoding, multi-head attention, layer normalization, residual connections, and the encoder-decoder structure. Explain how these components work together for natural language processing tasks like translation, summarization, and question answering. Also discuss the computational complexity and memory requirements of transformers compared to RNNs and LSTMs.",very_long,89,55,470,0.5,7.5636363636363635,55,t5-small,60.0,1.1942251388973091e-05,42.99210500030313,1.719684200012125,4.776900555589237e-06,estimated,False,2025-12-04T17:17:25.114912
M0045,"Develop a detailed strategy for building a production-ready machine learning pipeline considering all stages from data ingestion and validation, through feature engineering and selection, model training with cross-validation, hyperparameter optimization using grid search or Bayesian methods, model evaluation with appropriate metrics, deployment using containerization and orchestration, monitoring for data drift and model degradation, and establishing feedback loops for continuous improvement.",very_long,85,60,497,0.5,7.3,60,t5-small,60.0,1.4725151389332799e-05,53.010545001598075,2.120421800063923,5.89006055573312e-06,estimated,False,2025-12-04T17:17:27.287585
M0046,"Create an exhaustive comparison of different deep learning frameworks including TensorFlow, PyTorch, JAX, and MXNet across multiple dimensions such as ease of use and learning curve, computational performance and optimization capabilities, distributed training support, mobile and edge deployment options, community support and ecosystem, debugging and profiling tools, and production deployment considerations. Provide specific examples and use cases where each framework excels. Additionally discuss the trade-offs between static and dynamic computation graphs and how this affects model development and debugging. Compare the performance of these frameworks on common benchmarks like ImageNet classification and GLUE language understanding tasks. Discuss the impact of hardware accelerators like GPUs and TPUs on framework choice and how each framework handles automatic mixed precision training for improved throughput.",extra_long,169,121,923,0.5,6.636363636363637,121,t5-small,60.0,1.988958402782575e-05,71.6025025001727,2.864100100006908,7.9558336111303e-06,estimated,False,2025-12-04T17:17:30.205016
M0047,"Analyze the complete lifecycle of a machine learning project from initial problem formulation and stakeholder requirements gathering, through data acquisition and quality assessment, exploratory data analysis and visualization, feature engineering and selection using statistical methods and domain knowledge, model selection comparing different algorithm families like linear models, tree-based ensembles, neural networks, and support vector machines, hyperparameter optimization strategies including random search, grid search, and Bayesian optimization, model evaluation with cross-validation and proper test set holdout, error analysis and debugging, model interpretation using SHAP values and LIME, deployment strategies for batch and real-time inference, monitoring for performance degradation and data drift, and establishing processes for model retraining and versioning. Also cover the importance of reproducibility through version control of data, code, and models, documentation best practices, and establishing clear communication channels between data scientists and stakeholders throughout the project.",extra_long,192,138,1116,0.5,7.094202898550725,138,t5-small,60.0,1.56184888886249e-05,56.22655999904964,2.2490623999619856,6.247395555449961e-06,estimated,False,2025-12-04T17:17:32.507007
M0048,"Provide an in-depth technical explanation of how large language models like GPT and BERT work, starting from the fundamental concepts of word embeddings and contextual representations, through the transformer architecture with its self-attention mechanism that allows tokens to attend to all other tokens in the sequence, the role of positional encoding in capturing sequence order information, how multi-head attention enables the model to focus on different aspects of the input simultaneously, the importance of layer normalization and residual connections for stable training of deep networks, the pre-training objectives like masked language modeling for BERT and causal language modeling for GPT, fine-tuning strategies for downstream tasks, techniques for efficient inference like quantization and pruning, and recent advances like instruction tuning and reinforcement learning from human feedback. Additionally explain the scaling laws that govern the relationship between model size, dataset size, and compute budget, and how these findings have influenced the development of increasingly larger models.",extra_long,202,154,1112,0.5,6.2272727272727275,154,t5-small,60.0,1.9886519443793482e-05,71.59146999765653,2.8636587999062613,7.954607777517393e-06,estimated,False,2025-12-04T17:17:35.425040
M0049,What is machine learning?,simple,6,4,25,0.5,5.5,4,t5-small,60.0,2.3210374994151707e-06,8.355734997894615,0.3342293999157846,9.284149997660683e-07,estimated,False,2025-12-04T17:17:35.810395
M0050,Define artificial intelligence.,simple,6,3,31,0.5,9.666666666666666,3,t5-small,60.0,1.3079993053300616e-06,4.708797499188222,0.18835189996752888,5.231997221320247e-07,estimated,False,2025-12-04T17:17:36.050255
M0051,Explain Python briefly.,simple,5,3,23,0.5,7.0,3,t5-small,60.0,1.3328840278619383e-06,4.798382500302978,0.19193530001211911,5.331536111447754e-07,estimated,False,2025-12-04T17:17:36.293634
M0052,How can I improve my Python programming skills effectively and become a better developer?,medium,17,14,89,0.5,5.428571428571429,14,t5-small,60.0,4.505116666324385e-06,16.218419998767786,0.6487367999507114,1.8020466665297543e-06,estimated,False,2025-12-04T17:17:36.993672
M0053,"Explain the differences between supervised learning, unsupervised learning, and reinforcement learning.",medium,17,11,103,0.5,8.454545454545455,11,t5-small,60.0,2.854983333640525e-06,10.27794000110589,0.4111176000442356,1.14199333345621e-06,estimated,False,2025-12-04T17:17:37.455935
M0054,"Explain how neural networks learn patterns from data through the process of backpropagation, including forward pass, loss calculation, gradient computation, and weight updates. Also describe how different activation functions like ReLU, sigmoid, and tanh affect the learning process.",long,58,38,283,0.5,6.473684210526316,38,t5-small,60.0,7.3808701393621355e-06,26.571132501703687,1.0628453000681475,2.9523480557448545e-06,estimated,False,2025-12-04T17:17:38.570095
M0055,"Describe the complete process of training a machine learning model from scratch, including data collection, preprocessing, feature engineering, model selection, hyperparameter tuning, cross-validation, training, evaluation, and deployment to production.",long,49,29,253,0.5,7.758620689655173,29,t5-small,60.0,8.361320138242769e-06,30.100752497673966,1.2040300999069586,3.3445280552971075e-06,estimated,False,2025-12-04T17:17:39.826506
M0056,"Provide a comprehensive analysis of the transformer architecture including self-attention mechanisms, positional encoding, multi-head attention, layer normalization, residual connections, and the encoder-decoder structure. Explain how these components work together for natural language processing tasks like translation, summarization, and question answering. Also discuss the computational complexity and memory requirements of transformers compared to RNNs and LSTMs.",very_long,89,55,470,0.5,7.5636363636363635,55,t5-small,60.0,1.1925498610556437e-05,42.93179499800317,1.7172717999201268,4.770199444222575e-06,estimated,False,2025-12-04T17:17:41.595657
M0057,"Develop a detailed strategy for building a production-ready machine learning pipeline considering all stages from data ingestion and validation, through feature engineering and selection, model training with cross-validation, hyperparameter optimization using grid search or Bayesian methods, model evaluation with appropriate metrics, deployment using containerization and orchestration, monitoring for data drift and model degradation, and establishing feedback loops for continuous improvement.",very_long,85,60,497,0.5,7.3,60,t5-small,60.0,1.4985161111528417e-05,53.9465800015023,2.157863200060092,5.994064444611367e-06,estimated,False,2025-12-04T17:17:43.805269
M0058,"Create an exhaustive comparison of different deep learning frameworks including TensorFlow, PyTorch, JAX, and MXNet across multiple dimensions such as ease of use and learning curve, computational performance and optimization capabilities, distributed training support, mobile and edge deployment options, community support and ecosystem, debugging and profiling tools, and production deployment considerations. Provide specific examples and use cases where each framework excels. Additionally discuss the trade-offs between static and dynamic computation graphs and how this affects model development and debugging. Compare the performance of these frameworks on common benchmarks like ImageNet classification and GLUE language understanding tasks. Discuss the impact of hardware accelerators like GPUs and TPUs on framework choice and how each framework handles automatic mixed precision training for improved throughput.",extra_long,169,121,923,0.5,6.636363636363637,121,t5-small,60.0,2.020666249963041e-05,72.74398499866948,2.909759399946779,8.082664999852165e-06,estimated,False,2025-12-04T17:17:46.768465
M0059,"Analyze the complete lifecycle of a machine learning project from initial problem formulation and stakeholder requirements gathering, through data acquisition and quality assessment, exploratory data analysis and visualization, feature engineering and selection using statistical methods and domain knowledge, model selection comparing different algorithm families like linear models, tree-based ensembles, neural networks, and support vector machines, hyperparameter optimization strategies including random search, grid search, and Bayesian optimization, model evaluation with cross-validation and proper test set holdout, error analysis and debugging, model interpretation using SHAP values and LIME, deployment strategies for batch and real-time inference, monitoring for performance degradation and data drift, and establishing processes for model retraining and versioning. Also cover the importance of reproducibility through version control of data, code, and models, documentation best practices, and establishing clear communication channels between data scientists and stakeholders throughout the project.",extra_long,192,138,1116,0.5,7.094202898550725,138,t5-small,60.0,1.554387361102272e-05,55.957944999681786,2.2383177999872714,6.217549444409088e-06,estimated,False,2025-12-04T17:17:49.060704
M0060,"Provide an in-depth technical explanation of how large language models like GPT and BERT work, starting from the fundamental concepts of word embeddings and contextual representations, through the transformer architecture with its self-attention mechanism that allows tokens to attend to all other tokens in the sequence, the role of positional encoding in capturing sequence order information, how multi-head attention enables the model to focus on different aspects of the input simultaneously, the importance of layer normalization and residual connections for stable training of deep networks, the pre-training objectives like masked language modeling for BERT and causal language modeling for GPT, fine-tuning strategies for downstream tasks, techniques for efficient inference like quantization and pruning, and recent advances like instruction tuning and reinforcement learning from human feedback. Additionally explain the scaling laws that govern the relationship between model size, dataset size, and compute budget, and how these findings have influenced the development of increasingly larger models.",extra_long,202,154,1112,0.5,6.2272727272727275,154,t5-small,60.0,2.027243888894898e-05,72.98078000021633,2.919231200008653,8.108975555579593e-06,estimated,False,2025-12-04T17:17:52.034127
M0061,What is machine learning?,simple,6,4,25,0.5,5.5,4,t5-small,60.0,2.43883333314443e-06,8.779799999319948,0.35119199997279793,9.755333332577722e-07,estimated,False,2025-12-04T17:17:52.436690
M0062,Define artificial intelligence.,simple,6,3,31,0.5,9.666666666666666,3,t5-small,60.0,1.2883847222029646e-06,4.638184999930672,0.1855273999972269,5.153538888811858e-07,estimated,False,2025-12-04T17:17:52.673013
M0063,Explain Python briefly.,simple,5,3,23,0.5,7.0,3,t5-small,60.0,9.872034724038612e-07,3.5539325006539,0.142157300026156,3.948813889615445e-07,estimated,False,2025-12-04T17:17:52.866139
M0064,How can I improve my Python programming skills effectively and become a better developer?,medium,17,14,89,0.5,5.428571428571429,14,t5-small,60.0,4.357126388640608e-06,15.685654999106191,0.6274261999642476,1.7428505554562435e-06,estimated,False,2025-12-04T17:17:53.545732
M0065,"Explain the differences between supervised learning, unsupervised learning, and reinforcement learning.",medium,17,11,103,0.5,8.454545454545455,11,t5-small,60.0,3.000039583841701e-06,10.800142501830123,0.43200570007320493,1.2000158335366806e-06,estimated,False,2025-12-04T17:17:54.029305
M0066,"Explain how neural networks learn patterns from data through the process of backpropagation, including forward pass, loss calculation, gradient computation, and weight updates. Also describe how different activation functions like ReLU, sigmoid, and tanh affect the learning process.",long,58,38,283,0.5,6.473684210526316,38,t5-small,60.0,7.4047659727511925e-06,26.657157501904294,1.0662863000761718,2.9619063891004773e-06,estimated,False,2025-12-04T17:17:55.147783
M0067,"Describe the complete process of training a machine learning model from scratch, including data collection, preprocessing, feature engineering, model selection, hyperparameter tuning, cross-validation, training, evaluation, and deployment to production.",long,49,29,253,0.5,7.758620689655173,29,t5-small,60.0,8.746240971959196e-06,31.486467499053106,1.2594586999621242,3.4984963887836788e-06,estimated,False,2025-12-04T17:17:56.459232
M0068,"Provide a comprehensive analysis of the transformer architecture including self-attention mechanisms, positional encoding, multi-head attention, layer normalization, residual connections, and the encoder-decoder structure. Explain how these components work together for natural language processing tasks like translation, summarization, and question answering. Also discuss the computational complexity and memory requirements of transformers compared to RNNs and LSTMs.",very_long,89,55,470,0.5,7.5636363636363635,55,t5-small,60.0,1.213243819398081e-05,43.67677749833092,1.7470710999332368,4.852975277592325e-06,estimated,False,2025-12-04T17:17:58.259092
M0069,"Develop a detailed strategy for building a production-ready machine learning pipeline considering all stages from data ingestion and validation, through feature engineering and selection, model training with cross-validation, hyperparameter optimization using grid search or Bayesian methods, model evaluation with appropriate metrics, deployment using containerization and orchestration, monitoring for data drift and model degradation, and establishing feedback loops for continuous improvement.",very_long,85,60,497,0.5,7.3,60,t5-small,60.0,1.5972164583217818e-05,57.499792499584146,2.299991699983366,6.388865833287127e-06,estimated,False,2025-12-04T17:18:00.612656
M0070,"Create an exhaustive comparison of different deep learning frameworks including TensorFlow, PyTorch, JAX, and MXNet across multiple dimensions such as ease of use and learning curve, computational performance and optimization capabilities, distributed training support, mobile and edge deployment options, community support and ecosystem, debugging and profiling tools, and production deployment considerations. Provide specific examples and use cases where each framework excels. Additionally discuss the trade-offs between static and dynamic computation graphs and how this affects model development and debugging. Compare the performance of these frameworks on common benchmarks like ImageNet classification and GLUE language understanding tasks. Discuss the impact of hardware accelerators like GPUs and TPUs on framework choice and how each framework handles automatic mixed precision training for improved throughput.",extra_long,169,121,923,0.5,6.636363636363637,121,t5-small,60.0,1.905079999980646e-05,68.58287999930326,2.7433151999721304,7.620319999922585e-06,estimated,False,2025-12-04T17:18:03.410146
M0071,"Analyze the complete lifecycle of a machine learning project from initial problem formulation and stakeholder requirements gathering, through data acquisition and quality assessment, exploratory data analysis and visualization, feature engineering and selection using statistical methods and domain knowledge, model selection comparing different algorithm families like linear models, tree-based ensembles, neural networks, and support vector machines, hyperparameter optimization strategies including random search, grid search, and Bayesian optimization, model evaluation with cross-validation and proper test set holdout, error analysis and debugging, model interpretation using SHAP values and LIME, deployment strategies for batch and real-time inference, monitoring for performance degradation and data drift, and establishing processes for model retraining and versioning. Also cover the importance of reproducibility through version control of data, code, and models, documentation best practices, and establishing clear communication channels between data scientists and stakeholders throughout the project.",extra_long,192,138,1116,0.5,7.094202898550725,138,t5-small,60.0,1.605978749931738e-05,57.815234997542575,2.312609399901703,6.423914999726953e-06,estimated,False,2025-12-04T17:18:05.776737
M0072,"Provide an in-depth technical explanation of how large language models like GPT and BERT work, starting from the fundamental concepts of word embeddings and contextual representations, through the transformer architecture with its self-attention mechanism that allows tokens to attend to all other tokens in the sequence, the role of positional encoding in capturing sequence order information, how multi-head attention enables the model to focus on different aspects of the input simultaneously, the importance of layer normalization and residual connections for stable training of deep networks, the pre-training objectives like masked language modeling for BERT and causal language modeling for GPT, fine-tuning strategies for downstream tasks, techniques for efficient inference like quantization and pruning, and recent advances like instruction tuning and reinforcement learning from human feedback. Additionally explain the scaling laws that govern the relationship between model size, dataset size, and compute budget, and how these findings have influenced the development of increasingly larger models.",extra_long,202,154,1112,0.5,6.2272727272727275,154,t5-small,60.0,2.1735402778075593e-05,78.24745000107214,3.1298980000428855,8.694161111230238e-06,estimated,False,2025-12-04T17:18:08.960558
M0073,What is machine learning?,simple,6,4,25,0.5,5.5,4,t5-small,60.0,2.9537979167394547e-06,10.633672500262037,0.4253469000104815,1.181519166695782e-06,estimated,False,2025-12-04T17:18:09.437664
M0074,Define artificial intelligence.,simple,6,3,31,0.5,9.666666666666666,3,t5-small,60.0,1.7346138888064565e-06,6.244609999703243,0.24978439998812973,6.938455555225827e-07,estimated,False,2025-12-04T17:18:09.740570
M0075,Explain Python briefly.,simple,5,3,23,0.5,7.0,3,t5-small,60.0,1.1525729173121766e-06,4.149262502323836,0.16597050009295344,4.6102916692487065e-07,estimated,False,2025-12-04T17:18:09.958482
M0076,How can I improve my Python programming skills effectively and become a better developer?,medium,17,14,89,0.5,5.428571428571429,14,t5-small,60.0,5.584282638867282e-06,20.103417499922216,0.8041366999968886,2.233713055546913e-06,estimated,False,2025-12-04T17:18:10.814669
M0077,"Explain the differences between supervised learning, unsupervised learning, and reinforcement learning.",medium,17,11,103,0.5,8.454545454545455,11,t5-small,60.0,2.958117360928251e-06,10.649222499341704,0.42596889997366816,1.1832469443713005e-06,estimated,False,2025-12-04T17:18:11.292434
M0078,"Explain how neural networks learn patterns from data through the process of backpropagation, including forward pass, loss calculation, gradient computation, and weight updates. Also describe how different activation functions like ReLU, sigmoid, and tanh affect the learning process.",long,58,38,283,0.5,6.473684210526316,38,t5-small,60.0,7.392034722013503e-06,26.61132499924861,1.0644529999699444,2.956813888805401e-06,estimated,False,2025-12-04T17:18:12.408509
M0079,"Describe the complete process of training a machine learning model from scratch, including data collection, preprocessing, feature engineering, model selection, hyperparameter tuning, cross-validation, training, evaluation, and deployment to production.",long,49,29,253,0.5,7.758620689655173,29,t5-small,60.0,8.609284721993996e-06,30.993424999178387,1.2397369999671355,3.4437138887975986e-06,estimated,False,2025-12-04T17:18:13.699890
M0080,"Provide a comprehensive analysis of the transformer architecture including self-attention mechanisms, positional encoding, multi-head attention, layer normalization, residual connections, and the encoder-decoder structure. Explain how these components work together for natural language processing tasks like translation, summarization, and question answering. Also discuss the computational complexity and memory requirements of transformers compared to RNNs and LSTMs.",very_long,89,55,470,0.5,7.5636363636363635,55,t5-small,60.0,1.2747545138861622e-05,45.89116249990184,1.8356464999960735,5.099018055544649e-06,estimated,False,2025-12-04T17:18:15.588510
M0081,"Develop a detailed strategy for building a production-ready machine learning pipeline considering all stages from data ingestion and validation, through feature engineering and selection, model training with cross-validation, hyperparameter optimization using grid search or Bayesian methods, model evaluation with appropriate metrics, deployment using containerization and orchestration, monitoring for data drift and model degradation, and establishing feedback loops for continuous improvement.",very_long,85,60,497,0.5,7.3,60,t5-small,60.0,1.5285884722187702e-05,55.029184999875724,2.201167399995029,6.1143538888750815e-06,estimated,False,2025-12-04T17:18:17.842278
M0082,"Create an exhaustive comparison of different deep learning frameworks including TensorFlow, PyTorch, JAX, and MXNet across multiple dimensions such as ease of use and learning curve, computational performance and optimization capabilities, distributed training support, mobile and edge deployment options, community support and ecosystem, debugging and profiling tools, and production deployment considerations. Provide specific examples and use cases where each framework excels. Additionally discuss the trade-offs between static and dynamic computation graphs and how this affects model development and debugging. Compare the performance of these frameworks on common benchmarks like ImageNet classification and GLUE language understanding tasks. Discuss the impact of hardware accelerators like GPUs and TPUs on framework choice and how each framework handles automatic mixed precision training for improved throughput.",extra_long,169,121,923,0.5,6.636363636363637,121,t5-small,60.0,1.8565393055420523e-05,66.83541499951389,2.6734165999805555,7.42615722216821e-06,estimated,False,2025-12-04T17:18:20.568463
M0083,"Analyze the complete lifecycle of a machine learning project from initial problem formulation and stakeholder requirements gathering, through data acquisition and quality assessment, exploratory data analysis and visualization, feature engineering and selection using statistical methods and domain knowledge, model selection comparing different algorithm families like linear models, tree-based ensembles, neural networks, and support vector machines, hyperparameter optimization strategies including random search, grid search, and Bayesian optimization, model evaluation with cross-validation and proper test set holdout, error analysis and debugging, model interpretation using SHAP values and LIME, deployment strategies for batch and real-time inference, monitoring for performance degradation and data drift, and establishing processes for model retraining and versioning. Also cover the importance of reproducibility through version control of data, code, and models, documentation best practices, and establishing clear communication channels between data scientists and stakeholders throughout the project.",extra_long,192,138,1116,0.5,7.094202898550725,138,t5-small,60.0,1.571901458292915e-05,56.58845249854494,2.2635380999417976,6.287605833171661e-06,estimated,False,2025-12-04T17:18:22.885561
M0084,"Provide an in-depth technical explanation of how large language models like GPT and BERT work, starting from the fundamental concepts of word embeddings and contextual representations, through the transformer architecture with its self-attention mechanism that allows tokens to attend to all other tokens in the sequence, the role of positional encoding in capturing sequence order information, how multi-head attention enables the model to focus on different aspects of the input simultaneously, the importance of layer normalization and residual connections for stable training of deep networks, the pre-training objectives like masked language modeling for BERT and causal language modeling for GPT, fine-tuning strategies for downstream tasks, techniques for efficient inference like quantization and pruning, and recent advances like instruction tuning and reinforcement learning from human feedback. Additionally explain the scaling laws that govern the relationship between model size, dataset size, and compute budget, and how these findings have influenced the development of increasingly larger models.",extra_long,202,154,1112,0.5,6.2272727272727275,154,t5-small,60.0,2.1220751388884513e-05,76.39470499998424,3.0557881999993697,8.488300555553806e-06,estimated,False,2025-12-04T17:18:25.996118
M0085,What is machine learning?,simple,6,4,25,0.5,5.5,4,t5-small,60.0,2.53394930526459e-06,9.122217498952523,0.3648886999581009,1.013579722105836e-06,estimated,False,2025-12-04T17:18:26.412261
M0086,Define artificial intelligence.,simple,6,3,31,0.5,9.666666666666666,3,t5-small,60.0,1.1948409721501069e-06,4.3014274997403845,0.17205709998961538,4.779363888600428e-07,estimated,False,2025-12-04T17:18:26.635656
M0087,Explain Python briefly.,simple,5,3,23,0.5,7.0,3,t5-small,60.0,9.890062501654029e-07,3.5604225005954504,0.14241690002381802,3.9560250006616115e-07,estimated,False,2025-12-04T17:18:26.829864
M0088,How can I improve my Python programming skills effectively and become a better developer?,medium,17,14,89,0.5,5.428571428571429,14,t5-small,60.0,4.2619694447946835e-06,15.343090001260862,0.6137236000504345,1.7047877779178735e-06,estimated,False,2025-12-04T17:18:27.495482
M0089,"Explain the differences between supervised learning, unsupervised learning, and reinforcement learning.",medium,17,11,103,0.5,8.454545454545455,11,t5-small,60.0,2.9673937501178847e-06,10.682617500424385,0.4273047000169754,1.186957500047154e-06,estimated,False,2025-12-04T17:18:27.974862
M0090,"Explain how neural networks learn patterns from data through the process of backpropagation, including forward pass, loss calculation, gradient computation, and weight updates. Also describe how different activation functions like ReLU, sigmoid, and tanh affect the learning process.",long,58,38,283,0.5,6.473684210526316,38,t5-small,60.0,7.090479861492188e-06,25.525727501371875,1.021029100054875,2.8361919445968754e-06,estimated,False,2025-12-04T17:18:29.047419
M0091,"Describe the complete process of training a machine learning model from scratch, including data collection, preprocessing, feature engineering, model selection, hyperparameter tuning, cross-validation, training, evaluation, and deployment to production.",long,49,29,253,0.5,7.758620689655173,29,t5-small,60.0,8.298334722187266e-06,29.87400499987416,1.1949601999949664,3.319333888874907e-06,estimated,False,2025-12-04T17:18:30.294271
M0092,"Provide a comprehensive analysis of the transformer architecture including self-attention mechanisms, positional encoding, multi-head attention, layer normalization, residual connections, and the encoder-decoder structure. Explain how these components work together for natural language processing tasks like translation, summarization, and question answering. Also discuss the computational complexity and memory requirements of transformers compared to RNNs and LSTMs.",very_long,89,55,470,0.5,7.5636363636363635,55,t5-small,60.0,1.3497749999967507e-05,48.591899999883026,1.943675999995321,5.399099999987004e-06,estimated,False,2025-12-04T17:18:32.291019
M0093,"Develop a detailed strategy for building a production-ready machine learning pipeline considering all stages from data ingestion and validation, through feature engineering and selection, model training with cross-validation, hyperparameter optimization using grid search or Bayesian methods, model evaluation with appropriate metrics, deployment using containerization and orchestration, monitoring for data drift and model degradation, and establishing feedback loops for continuous improvement.",very_long,85,60,497,0.5,7.3,60,t5-small,60.0,1.4311631249761881e-05,51.52187249914277,2.060874899965711,5.7246524999047525e-06,estimated,False,2025-12-04T17:18:34.405035
M0094,"Create an exhaustive comparison of different deep learning frameworks including TensorFlow, PyTorch, JAX, and MXNet across multiple dimensions such as ease of use and learning curve, computational performance and optimization capabilities, distributed training support, mobile and edge deployment options, community support and ecosystem, debugging and profiling tools, and production deployment considerations. Provide specific examples and use cases where each framework excels. Additionally discuss the trade-offs between static and dynamic computation graphs and how this affects model development and debugging. Compare the performance of these frameworks on common benchmarks like ImageNet classification and GLUE language understanding tasks. Discuss the impact of hardware accelerators like GPUs and TPUs on framework choice and how each framework handles automatic mixed precision training for improved throughput.",extra_long,169,121,923,0.5,6.636363636363637,121,t5-small,60.0,1.8958545833205183e-05,68.25076499953866,2.7300305999815464,7.583418333282073e-06,estimated,False,2025-12-04T17:18:37.187923
M0095,"Analyze the complete lifecycle of a machine learning project from initial problem formulation and stakeholder requirements gathering, through data acquisition and quality assessment, exploratory data analysis and visualization, feature engineering and selection using statistical methods and domain knowledge, model selection comparing different algorithm families like linear models, tree-based ensembles, neural networks, and support vector machines, hyperparameter optimization strategies including random search, grid search, and Bayesian optimization, model evaluation with cross-validation and proper test set holdout, error analysis and debugging, model interpretation using SHAP values and LIME, deployment strategies for batch and real-time inference, monitoring for performance degradation and data drift, and establishing processes for model retraining and versioning. Also cover the importance of reproducibility through version control of data, code, and models, documentation best practices, and establishing clear communication channels between data scientists and stakeholders throughout the project.",extra_long,192,138,1116,0.5,7.094202898550725,138,t5-small,60.0,1.64258062504814e-05,59.13290250173304,2.3653161000693217,6.57032250019256e-06,estimated,False,2025-12-04T17:18:39.607581
M0096,"Provide an in-depth technical explanation of how large language models like GPT and BERT work, starting from the fundamental concepts of word embeddings and contextual representations, through the transformer architecture with its self-attention mechanism that allows tokens to attend to all other tokens in the sequence, the role of positional encoding in capturing sequence order information, how multi-head attention enables the model to focus on different aspects of the input simultaneously, the importance of layer normalization and residual connections for stable training of deep networks, the pre-training objectives like masked language modeling for BERT and causal language modeling for GPT, fine-tuning strategies for downstream tasks, techniques for efficient inference like quantization and pruning, and recent advances like instruction tuning and reinforcement learning from human feedback. Additionally explain the scaling laws that govern the relationship between model size, dataset size, and compute budget, and how these findings have influenced the development of increasingly larger models.",extra_long,202,154,1112,0.5,6.2272727272727275,154,t5-small,60.0,2.029679375037732e-05,73.06845750135835,2.922738300054334,8.118717500150928e-06,estimated,False,2025-12-04T17:18:42.584641
M0097,What is machine learning?,simple,6,4,25,0.5,5.5,4,t5-small,60.0,1.9414708327126897e-06,6.989294997765683,0.2795717999106273,7.765883330850759e-07,estimated,False,2025-12-04T17:18:42.915392
M0098,Define artificial intelligence.,simple,6,3,31,0.5,9.666666666666666,3,t5-small,60.0,1.1469381942232656e-06,4.1289774992037565,0.16515909996815026,4.5877527768930626e-07,estimated,False,2025-12-04T17:18:43.131691
M0099,Explain Python briefly.,simple,5,3,23,0.5,7.0,3,t5-small,60.0,9.532798616823534e-07,3.431807502056472,0.13727230008225888,3.813119446729414e-07,estimated,False,2025-12-04T17:18:43.320531
M0100,How can I improve my Python programming skills effectively and become a better developer?,medium,17,14,89,0.5,5.428571428571429,14,t5-small,60.0,4.3101527779880495e-06,15.516550000756979,0.6206620000302792,1.72406111119522e-06,estimated,False,2025-12-04T17:18:43.992936
